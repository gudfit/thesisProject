# --- Configuration for the GPT-2 Medium Pruning Experiment ---

# --- Model & Dataset Setup ---
base_model_id: "gpt2-medium"
dataset_name: "wikitext"
dataset_subset: "wikitext-2-raw-v1"
test_split: "validation"
finetuned_base_path: "./finetuned_models/gpt2-medium"
pruned_models_dir: "./pruned_models/gpt2-medium"

# --- Pruning Parameters ---
pruning_amounts: [0.2, 0.4, 0.6, 0.8] # Prune 20%, 40%, 60%, 80%

# --- Experiment Parameters ---
experiment_name: "gpt2_medium_pruning_experiment"
theta_budgets: [5, 10, 20, 40, 100] # Use the prompt lengths from your final results
num_repetitions: 1
semantic_threshold: 0.95
