experiment_name: "gpt2_medium_pruning_analysis"
base_model_id: "gpt2-medium"
dataset_name: "wikitext"
dataset_subset: "wikitext-2-raw-v1"
test_split: "validation"

finetuned_base_path: "./models/finetuned/gpt2-medium"
pruned_models_dir: "./models/pruned/gpt2-medium"
output_dir: "./results"

pruning_amounts: [0.2, 0.4, 0.6, 0.8]
theta_budgets: [5, 10, 20, 40, 100]
num_repetitions: 3
semantic_threshold: 0.95
max_samples: 500
