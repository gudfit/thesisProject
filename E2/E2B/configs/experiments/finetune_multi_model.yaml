experiment_name: "multi_model_finetune_comparison"
dataset_name: "wikitext"
dataset_subset: "wikitext-2-raw-v1"
test_split: "validation"
ood_dataset_name: "ag_news"
ood_dataset_subset: "default"
ood_split: "test"
ood_hard_level: "mstr"
ood_hard_k: 6

lambda_budgets:
  - {name: "Cerebras-111M", model_id: "cerebras/Cerebras-GPT-111M"}
  - {name: "Cerebras-256M", model_id: "cerebras/Cerebras-GPT-256M"}
  - {name: "GPT2-small", model_id: "gpt2"}
  - {name: "GPT2-medium", model_id: "gpt2-medium"}

finetune_output_dir: "./models/finetuned"
output_dir: "./results"

training_args:
  num_train_epochs: 1
  per_device_train_batch_size: 4
  logging_steps: 200
  save_strategy: "no"

theta_budgets: [5, 10, 20, 40, 60, 100]
num_repetitions: 1
semantic_threshold: 0.95
semantic_threshold_ood: 0.85
max_samples: 100
max_samples_ood: 100

