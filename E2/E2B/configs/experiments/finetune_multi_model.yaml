experiment_name: "multi_model_finetune_comparison"
dataset_name: "wikitext"
dataset_subset: "wikitext-2-raw-v1"
test_split: "validation"

lambda_budgets:
  - {name: "Cerebras-111M", model_id: "cerebras/Cerebras-GPT-111M"}
  - {name: "Cerebras-256M", model_id: "cerebras/Cerebras-GPT-256M"}
  - {name: "GPT2-small", model_id: "gpt2"}
  - {name: "GPT2-medium", model_id: "gpt2-medium"}

finetune_output_dir: "./models/finetuned"
output_dir: "./results"

training_args:
  num_train_epochs: 1
  per_device_train_batch_size: 4
  logging_steps: 200
  save_strategy: "no"
  evaluation_strategy: "no"

theta_budgets: [5, 10, 20, 40, 60, 100]
num_repetitions: 3
semantic_threshold: 0.95
max_samples: 1000 
