dataset_name: "wikitext"
dataset_subset: "wikitext-2-raw-v1"
test_split: "validation" 
lambda_budgets:
  - {name: "Cerebras-111M", model_id: "cerebras/Cerebras-GPT-111M"}
  - {name: "Cerebras-256M", model_id: "cerebras/Cerebras-GPT-256M"}
  - {name: "Cerebras-590M", model_id: "cerebras/Cerebras-GPT-590M"}
  - {name: "GPT2-small",    model_id: "gpt2"}
  - {name: "GPT2-medium",   model_id: "gpt2-medium"}

finetune_output_dir: "./finetuned_models"
training_args:
  num_train_epochs: 1
  per_device_train_batch_size: 4
  logging_steps: 200

experiment_name: "final_finetuned_llm_experiment"
theta_budgets: [5, 10, 20, 40, 60, 100] 
num_repetitions: 1 
semantic_threshold: 0.95
