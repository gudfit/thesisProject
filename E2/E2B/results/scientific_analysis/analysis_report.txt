COMPRESSION METHOD ANALYSIS REPORT
================================================================================

KEY FINDINGS
----------------------------------------

BEST CONFIGURATIONS BY ARCHITECTURE:

Cerebras-111M:
  Best Similarity: Cerebras-111M (fine_tuning) - 1.0000
  Most Efficient: Cerebras-111M_b8_s50 (quantization) - 4.41 sim/GB

Cerebras-256M:
  Best Similarity: Cerebras-256M (fine_tuning) - 1.0000
  Most Efficient: Cerebras-256M (fine_tuning) - 0.97 sim/GB

GPT2-Small:
  Best Similarity: GPT2-small (fine_tuning) - 1.0000
  Most Efficient: GPT2-small_b4_s50 (quantization) - 3.94 sim/GB

GPT2-Medium:
  Best Similarity: GPT2-medium (fine_tuning) - 1.0000
  Most Efficient: GPT2-medium_b4_s30 (quantization) - 1.40 sim/GB


================================================================================
Note: Scaling laws are only fitted within the same architecture
      with varying compression levels (scientifically valid).
