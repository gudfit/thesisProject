models:
  bert:
    name: "bert-base-uncased"
    model_type: "bert"
    max_length: 512
  roberta:
    name: "roberta-base"
    model_type: "roberta"
    max_length: 512
  distilbert:
    name: "distilbert-base-uncased"
    model_type: "distilbert"
    max_length: 512

compression:
  masking_probabilities: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]
  quantization_bits: 8
  batch_size: 16

training:
  epochs: 50
  learning_rate: 5e-5
  warmup_steps: 500
  weight_decay: 0.01
  gradient_accumulation_steps: 2
  fp16: true

data:
  train_size: 10000
  test_size: 1000
  max_length: 256
  dataset_name: "wikitext"
  dataset_config: "wikitext-2-raw-v1"

evaluation:
  metrics:
    - "reconstruction_accuracy"
    - "perplexity"
    - "rouge"
    - "bert_score"
    - "semantic_similarity"
  save_reconstructions: true

experiment:
  seed: 42
  num_runs: 3
  device: "cuda"
  output_dir: "./results"
  save_models: true
  logging_steps: 100

visualization:
  figure_format: "png"
  dpi: 300
  style: "seaborn-v0_8-darkgrid"
  save_figures: true

experiment_1b:
  corpus_size: 5000
  test_size: 500
  factual_probes: 1000
  oracle_model: "gpt2-large"

  tolerance_thresholds:
    high_fidelity_factual: 0.95
    creative_reasoning: 0.85
    acceptable_degradation: 0.10

  fine_tune_sizes: [10, 50, 100, 500, 1000, 5000]
  target_performance: 0.80
  
experiment_1c:
  pruning:
    levels: [0.0, 0.1, 0.3, 0.5, 0.7, 0.9]
    type: "magnitude"  # or "structured"
    
  glue:
    tasks: ["sst2", "mrpc", "rte"]
    train_samples: 500
    eval_samples: 200
    fine_tune_epochs: 100
    
  benchmark:
    batch_size: 8
    num_inference_runs: 50
    
  structural_fidelity:
    test_samples: 20
    masking_probability: 0.3
    
  sweet_spot_weights:
    downstream_performance: 0.4
    structural_fidelity: 0.3
    computational_efficiency: 0.3


experiment_1d:
  num_samples: 1000  
  
  ar_models: ["gpt2", "gpt2-medium"]  
  
  pm_masking_levels: [0.3, 0.5, 0.7]  
  lsq_bits: 8  
  
  max_text_length: 2048  
  batch_size: 8

experiment_2a:
  output_dir: "results/experiment_2a"
  num_test_samples: 200 
  semantic_threshold: 0.95
  retrieval_budgets: [40, 50, 60, 70, 80] 
  pruning_base_model: "Llama-2-7B" 
  pruning_levels: []
  models:
    - name: "Llama-2-7B"
      path: "meta-llama/Llama-2-7b-hf"
      quantization: "half" 
    - name: "Llama-2-13B"
      path: "meta-llama/Llama-2-13b-hf"
      quantization: "half"
    - name: "Llama-2-7B-4bit"
      path: "meta-llama/Llama-2-7b-hf"
      quantization: "4bit" 
